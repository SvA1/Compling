{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW 3(1).ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SvA1/Compling/blob/master/HW_3(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPgRBVrboF0S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://raw.githubusercontent.com/mannefedov/compling_nlp_hse_course/master/data/correct_sents.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmJuQEsHoiaq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://raw.githubusercontent.com/mannefedov/compling_nlp_hse_course/master/data/sents_with_mistakes.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xcxHAtGok3_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://github.com/yutkin/Lenta.Ru-News-Dataset/releases/download/v1.0/lenta-ru-news.csv.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHqIxXz3ooOH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import gzip, csv\n",
        "from string import punctuation\n",
        "punctuation += \"«»—…“”\"\n",
        "punct = set(punctuation)\n",
        "from collections import Counter, defaultdict\n",
        "from pprint import pprint\n",
        "from nltk import sent_tokenize\n",
        "\n",
        "!pip install symspellpy\n",
        "from symspellpy import SymSpell, Verbosity\n",
        "import pkg_resources"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEYgutwVo6A1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = open('corpus_5000.txt', 'w')\n",
        "with gzip.open('lenta-ru-news.csv.gz', 'rt') as archive:\n",
        "    reader = csv.reader(archive, delimiter=',', quotechar='\"')\n",
        "    for i, line in enumerate(reader):\n",
        "        if i < 5000: \n",
        "            corpus.write(line[2].replace('\\xa0', ' ') + '\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jP4xOMSVpKoP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize(text):\n",
        "    \n",
        "    normalized_text = [(word.strip(punctuation)) for word \\\n",
        "                                                            in text.lower().split()]\n",
        "    normalized_text = [word for word in normalized_text if word]\n",
        "    return normalized_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HaGyPEZUpMF7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "corpus = []\n",
        "for text in open('corpus_5000.txt').read().splitlines()[:12000]:\n",
        "    sents = sent_tokenize(text)\n",
        "    norm_sents = [normalize(sent) for sent in sents]\n",
        "    corpus += norm_sents"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtfwTL7QpS6o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = []\n",
        "for text in open('corpus_5000.txt').read().splitlines():\n",
        "    sents = sent_tokenize(text)\n",
        "    norm_sents = [normalize(sent) for sent in sents]\n",
        "    corpus += norm_sents"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TcEvTLn6qvk0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
        "corpus_path = 'corpus_5000.txt'\n",
        "dictionary_path = sym_spell.create_dictionary(corpus_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJwtTThLvaAr",
        "colab_type": "code",
        "outputId": "eaffdb56-0db1-479a-b909-68fa35f907cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "def Sym_spell_func(input_term):\n",
        "  sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
        "  suggestions = sym_spell.lookup(input_term, Verbosity.CLOSEST,\n",
        "                               max_edit_distance=2, include_unknown=True)\n",
        "  return str(suggestions[0]).split(',')[0]\n",
        "Sym_spell_func('ооочень')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'очень'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15Wd95W4rpVs",
        "colab_type": "code",
        "outputId": "8d1a7d71-6966-4930-9316-10eb5a29b36f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "bad = open('sents_with_mistakes.txt', encoding='utf8').read().splitlines()\n",
        "true = open('correct_sents.txt', encoding='utf8').read().splitlines()\n",
        "print(bad[1])\n",
        "print(true[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Опофеозом дня для меня сегодня стала фраза услышанная в новостях:\n",
            "Апофеозом дня для меня сегодня стала фраза услышанная в новостях\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fv5qLLJo6lN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def align_words(sent_1, sent_2):\n",
        "    tokens_1 = sent_1.lower().split()\n",
        "    tokens_2 = sent_2.lower().split()\n",
        "    \n",
        "    tokens_1 = [re.sub('(^\\W+|\\W+$)', '', token) for token in tokens_1 if (set(token)-punct)]\n",
        "    tokens_2 = [re.sub('(^\\W+|\\W+$)', '', token) for token in tokens_2 if (set(token)-punct)]\n",
        "    \n",
        "    return list(zip(tokens_1, tokens_2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfMmu-Y9r0r-",
        "colab_type": "code",
        "outputId": "b57a0284-72ab-425f-8d22-7cf39fc46c27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        }
      },
      "source": [
        "pprint(align_words(true[1], bad[1]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('апофеозом', 'опофеозом'),\n",
            " ('дня', 'дня'),\n",
            " ('для', 'для'),\n",
            " ('меня', 'меня'),\n",
            " ('сегодня', 'сегодня'),\n",
            " ('стала', 'стала'),\n",
            " ('фраза', 'фраза'),\n",
            " ('услышанная', 'услышанная'),\n",
            " ('в', 'в'),\n",
            " ('новостях', 'новостях')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sClbim9-sAZN",
        "colab_type": "code",
        "outputId": "65e2bc94-5976-4a8b-8b7f-6676055dabfa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "print(correct/total)\n",
        "print(mistaken_fixed/total_mistaken)\n",
        "print(correct_broken/total_correct)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7020979020979021\n",
            "0.5003837298541827\n",
            "0.26771563110141267\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqLVHxkeska7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def deletes(word):\n",
        "    letters    = 'йцукенгшщзхъфывапролджэячсмитьбюё'\n",
        "    splits     = [(word[:i], word[i:])     for i in range(len(word) + 1)]\n",
        "    deletes1    = [L + R[1:]               for L, R in splits if R]\n",
        "    splits2     = [(word[:i], word[i:])    for i in range(len(deletes1))]\n",
        "    deletes2    = [L + R[2:]               for L, R in splits2 if R]\n",
        "    deletes = deletes1 + deletes2\n",
        "    return set(deletes)\n",
        "    print(\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LX3ftk9_tPDt",
        "colab_type": "code",
        "outputId": "4fd395fd-6446-4734-c49b-1b53a626462c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "corpus_sents = open('corpus_5000.txt', encoding='utf8').read().lower().split()\n",
        "true_list = [re.sub('(^\\W+|\\W+$)', '', token) for token in corpus_sents if (set(token)-punct)]\n",
        "\n",
        "true_dict = {}\n",
        "prob_dict = {}\n",
        "for i in true_list:\n",
        "  prob_dict[i] = prob_dict.get(i, 0) + 1\n",
        "  for j in deletes(i):\n",
        "    true_dict[j] =  i\n",
        "print(list(true_dict.keys())[:5])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['txt', 'tet', 'xt', 'tex', 'tt']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pf6lPV0ktxwf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prob(word):\n",
        "  p = prob_dict.get(word, 0)/len(set(true_list))\n",
        "  return(p)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9KUOIFJthqa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def in_pro(input_word):\n",
        "  candidates_list = []\n",
        "  local = {}\n",
        "  \n",
        "  if input_word in prob_dict:\n",
        "    return input_word\n",
        "  if input_word in true_dict:\n",
        "    candidates_list.append(true_dict[input_word])\n",
        "  for j in deletes(input_word):\n",
        "    if j in true_dict:\n",
        "        candidates_list.append(true_dict[j])\n",
        "        candidates_list = list(set(candidates_list))\n",
        "  if len(candidates_list) > 1:\n",
        "    for i in set(candidates_list):\n",
        "      local[prob(i)] = i\n",
        "      sorted_probs = sorted(list(local.keys()))\n",
        "      return local[sorted_probs[0]]\n",
        "  else:\n",
        "    if len(candidates_list) > 0:  \n",
        "     return candidates_list[0]\n",
        "    else:\n",
        "      return input_word + \"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBetQrw1t1LA",
        "colab_type": "code",
        "outputId": "0db14d75-0639-4b36-cc04-05f63dfdf43e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "print(in_pro('опофеозом'))\n",
        "print(in_pro('надаело'))\n",
        "print(in_pro('оочень'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "апофеозом\n",
            "надоело\n",
            "огонь\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgjWsCfBuI2M",
        "colab_type": "code",
        "outputId": "5030aac4-3333-41a0-98e6-fed8b70b43f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "print(correct/total)\n",
        "print(mistaken_fixed/total_mistaken)\n",
        "print(correct_broken/total_correct)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7020979020979021\n",
            "0.5003837298541827\n",
            "0.26771563110141267\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qugzIznr2vhA",
        "colab_type": "text"
      },
      "source": [
        "### Триграммная модель"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlkirrvGCXLd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus_news = [['<start>', '<start>'] + sent + ['<end>'] for sent in corpus]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JweKqJNu27eC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = []\n",
        "for text in open('corpus_5000.txt').read().splitlines():\n",
        "    sents = sent_tokenize(text)\n",
        "    norm_sents = [normalize(sent) for sent in sents]\n",
        "    corpus += norm_sents"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFLigV-421S4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ngrammer(tokens, n=2):\n",
        "    ngrams = []\n",
        "    for i in range(0,len(tokens)-n+1):\n",
        "        ngrams.append(' '.join(tokens[i:i+n]))\n",
        "    return ngrams"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVCxxzgM3JYP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "unigrams = Counter()\n",
        "bigrams = Counter()\n",
        "trigrams = Counter()\n",
        "\n",
        "for sentence in corpus_news:\n",
        "    unigrams.update(sentence)\n",
        "    bigrams.update(ngrammer(sentence))\n",
        "    trigrams.update(ngrammer(sentence, n=3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSGRbs2rCMk_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "id2word_unigrams = list(unigrams)\n",
        "word2id_unigrams = {word:i for i, word in enumerate(id2word_unigrams)}\n",
        "\n",
        "id2word_bigrams = list(bigrams)\n",
        "word2id_bigrams = {word:i for i, word in enumerate(id2word_bigrams)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zo3IgCDVCukx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7Lq-9w2ANYM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_closest_match_vec(text, X, vec, topn=20):\n",
        "    # превращаем слово в вектор такой же размерности\n",
        "    v = vec.transform([text])\n",
        "    \n",
        "    # вся эффективноть берется из того, что мы сразу считаем близость \n",
        "    # 1 вектора ко всей матрице (словам в словаре)\n",
        "    # считать по отдельности циклом было бы дольше\n",
        "    # вместо одного вектора может даже целая матрица\n",
        "    # тогда считаться в итоге будет ещё быстрее\n",
        "    \n",
        "    similarities = cosine_distances(v, X)[0] #distance - чем больше, тем хуже, а similarity наоборот\n",
        "    topn = similarities.argsort()[:topn] \n",
        "    \n",
        "    return [(id2word[top], similarities[top]) for top in topn]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AsGpsTcP_9Kz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_closest_hybrid_match(text, X, vec, topn=5, metric=textdistance.damerau_levenshtein):\n",
        "    # ваш код здесь\n",
        "    candidates = get_closest_match_vec(text, X, vec, topn*4)\n",
        "    sims = Counter()\n",
        "    lookup = [cand[0] for cand in candidates]\n",
        "    closest = get_closest_match_with_metric(text, lookup,topn, metric=metric)\n",
        "    \n",
        "    return closest\n",
        "\n",
        "get_closest_hybrid_match('сонце', X, vec)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}